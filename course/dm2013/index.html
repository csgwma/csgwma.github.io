<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Machine Learning and Knowledge Discovery</title>
</head>

<body style="width:960px; margin: 0 auto;">
<table id="header"> 
	<tbody><tr> 
		<td class="hspace"></td> 
		<td><img src="./ustcblue.jpg" width="200" height="200"></td>
        <td class="hspace2"></td> 
		<td>
			<div class="coursetitle"><b><font size="5">CS05141: Machine Learning and Knowledge Discovery</font></b></div><b> 
			<font size="4">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Fall 2013<br><br>
				<b><font size="5"><a href="http://staff.ustc.edu.cn/~cheneh/" rel="external" class="lecturer">Enhong Chen</a></font></b><br> 
                 School of Computer Science and Technology, USTC
            </font>
        </b></td> 
    </tr>  
</tbody></table>


<hr>
<h3>Teaching Assistant:</h3>
<li>Guowei Ma: gwma@mail.ustc.edu.cn</li>
<li>Lili Zhou: zhoulili@mail.ustc.edu.cn</li>

<br>

<hr>
<h3>Lectures</h3>
<ul>
	<li>Lecture1: <a href="./resources/chapter1.pdf">Introduction</a> </li>
	<li>Lecture2: <a href="./resources/ch2-final.ppt">Data Preprocessing</a> </li>
	<li>Lecture3: <a href="./resources/ch3_association_rule.ppt">Association Rule</a> </li>
    <li>Lecture4: <a href="./resources/ch4_classification.ppt">Classification</a></li>
    <li>Lecture5: <a href="./resources/chap5_alternative_classification(final).ppt">Alternative Classification</a> New! </li>
    <li>Lecture6: <a href="./resources/chap6_basic_cluster_analysis.ppt">Cluster Analysis</a> New! </li>
	<br>
    <br>
    Slides:
    <li>2013-10-16 Lei Zhang :<a href="./resources/2013-10-16.pdf">Incorporating Occupancy into Frequent Pattern Mining</a></li>
    <li>2013-10-23 Lei Zhang :<a href="./resources/2013-10-23.pdf">Mining Frequent Patterns in Print Logs</a></li>
    <li>2013-12-16 Qi Liu :<a href="./resources/2013-12-16.pdf">A brief introduction to RecSys</a></li>
    
</ul>
Note: password for slides：CS05141<br>
<br>
<hr>
<h3>Homework Assignments</h3>
<p>Reference Book:  <a href="http://book.douban.com/subject/5377669/">Introduction to Data Mining</a></p>

Announcements: All the homework assignments are selected from the Reference Book.
<br>
<ul>
	<li>HW1:  Ch6: 2, 3, 4, 9, 11  </li>
    <li>HW2:  Ch6: 12, 14, 17  &nbsp&nbsp  Ch4: 2 </li>
    <li>HW3:  Ch4: 9</li>
    <li>HW4:  Ch5: 1, 4, 9, 17</li>
</ul>
<br>

<hr>
<h3>Project</h3>
<p>Announcements about projects</p>
<a href="./project_submition.html">Project Submit<a><br>
<br>
<h4>Programming Assignments.</h4>
Choose one of the listed three tasks as your work:
 <p><li>Classification</li><br>
 &nbsp;&nbsp; Implement  one or more classifiers, such as KNN, Bayes, Decision Tree, SVM, ANN et al,  and test it  with <a href="http://mlr.cs.umass.edu/ml/datasets/Iris">UCI Iris dataset</a><br>
   &nbsp;&nbsp;optional: Implement one of the ensemble methods (Adaboost recommended) and compare it with the basic classifiers
    <br>
 </p>
<p><li>Clustering</li><br>
  &nbsp;&nbsp;Implement  K-means or other clustering algorithms and  test it with data set <a href="http://archive.ics.uci.edu/ml/datasets/Protein+Data"> Protein-data</a>. Arbitrary data file from it is appropriate. Compare your result with the ground truth clusters, e.g. the true attribute classes defines a natural cluster.<br>
  &nbsp;&nbsp;optional: Implement one DBSCAN or Affinity Propagation like method and compare it with  K-means<br>
</p>
<p><li>Pattern Mining</li><br>
   &nbsp;&nbsp;Implement Apriori or FP-growth like frequent pattern mining algorithms and test it with <a href="http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29">UCI slice dataset</a>.  Only submitting the frequent pattern results is OK. The support or confidence thresholds are up to you.<br>
   &nbsp;&nbsp;optional: Implement one  prefixspan like algorithm and compare it with your previous implemented  algorithm<br>
  </p>
<hr>
<h4>Final Project</h4>
  You can choose any one of the projects listed  below：<br>
  <p><li><strong>School User Predict</strong></li><br>
    &nbsp;&nbsp;<a href="./projects/SchoolUserPredict.html">detail information</a> <br><br>
    &nbsp;&nbsp;Tips: feature selection or graph mining..
  </p>
   <p><li><strong>Keyword industry classification</strong></li><br>
    &nbsp;&nbsp;<a href="./projects/KeywordIndustryClassification.html">detail information</a>
  </p>
  <p><li><strong>Community Detection </strong>(Clustering or community of  network nodes)</li><br>
    &nbsp;&nbsp;Choose a community detection algorithm,  Newman's modularity based methods like Louvan Method, or graph cut  based method like spectral clustering are all possible solution. The more you investigate, the more methods you will be clear with.<br>
    &nbsp;&nbsp;Tips: At the final step, presenting your detected representative communities using  Python package Networkx or other network visualization software is encouraged.  <br>
    &nbsp;&nbsp;Analogy to topic sensitive page rank, you can also consider topic into community detection. Enron email corpus serves a good test-bed.
   <!-- 
    <p><li><strong>Topic Sensitive page-rank</strong></li><br>
  The original topic sensitive page rank algorithm introduced by Taher Haveliwala is rather primitive. You can try to think it over again, and incorporate your own ideas about influence or topic into this framework, which is based on stationary distribution of random walk on graphs.<br>
  Wikipedia page presents a good introduction to this issue. For more details, refer to the original paper of Taher Haveliwala.<br>
  Tips: WebKB is a good data source for this experiment. There are many papers citing the paper 'Topic Sensitive Page Rank'. After taking a look at them, you can find out a lot more resources.<br>
   </p>
    <p><li><strong>Recommender System</strong></li><br>
  Use the famous Movielens Datasets to design and validate your own recommender system. Following the well known also widely used evaluation criteria RMSE is encouraged.<br>
   Although there is a huge volume of literature focusing on this datasets,  try your best not to follow the existing methods.  Conquer this project with your own insight on this dataset. <br>
   For example, the existing methods for computing the similarity, heuristics measuring user's preference, between users can be experimented with much more alternatives. <br>
   If you choose Model-based methods, such as latent factor models, the state of art methods in this area, try your best to find  useful latent factors and validate them with your own implementation.
    </p>
<p><li><strong> Anti Spam</strong></li><br>
  Decades ago, spam emails for advertising disturbs end users very much. Thus the function of  anti spam become an essential part of an email system. The algorithm based on Naive Bayes invented by Paul Graham works surprisingly well for such an application, which is further investigated by many authors.  In the era of web 2.0, large amount of user generated content become prevalent on the Internet.  Low quality pages or even malicious pages emerge everyday and how to identify them for building better search engine also becomes an important problem. <br>
  In this part, you are required to design an algorithm to effectively tackle the problem of anti spam. Available datasets include the Ling-spam data set, <a href="http://archive.ics.uci.edu/ml/datasets/Spambase">UCI Spambase data set</a>, Yahoo research web spam data set. A lot of useful resources can also be found on these links. Have fun by yourself.
</p>
<p><li><strong>Kaggle competitions</strong></li><br>
  The kaggle platform is a well known website for data scientists to compete for the competitions, which are provided by the various institutions, such as companies, governments or researchers. You can take part in any one of these competitions as you like, and use this problem as your course project. <br>
  After registering this website, Download the data sets, design and implement your own solution and submit your results to platform. Then you will get your ranking on the leader board. If you do your job sufficiently good. you may even win the prizes provided by the vendors.
  Although this project is slightly different from the previous ones, you should also submit your proposal and final writeup separately. And the outline should be the same as the previous ones too. The competition ranking will not affect your grades provided that you finish the project with enough efforts and show that you have a good hand of data mining skills.<br></p> -->
  <p>
  <li><strong>Model comparisons</strong>(more mathematics, and less application background)</li><br>
  &nbsp;&nbsp;In consideration of the fact that some students may only interest in the mathematical part of this course, such as regression and classification, optimization, factor analysis and so on. Here is a project that focuses on models only.
  You are required to do model comparison between various model, including aspects such as theoretical analysis, code implementation, experimental validation etc. <br>
  &nbsp;&nbsp;The project requires you to do comparisons between two pairs of models, logistic regression and SVM, basic linear regression(least squares based) and ridge or lasso regularized regression. The submission's outline and requirement are the same as previous ones. Make sure to do projects with your own efforts. <br>
  Tips:  Weka and Rapidminer are very good software for you to become familiar with these models.
  </p>
  

  
  <p> </p><br><br>
</body></html>